{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4efccf8-c5f1-4384-9d12-52ad4d07b893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import datetime\n",
    "import dlt\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Data Ingestion\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Function to generate sample data\n",
    "def generate_data():\n",
    "    data = [\n",
    "        (1, \"A\"),\n",
    "        (2, \"B\"),\n",
    "        (3, \"C\")\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "    df = df.withColumn(\"timestamp\", lit(datetime.datetime.now()))\n",
    "    return df\n",
    "    \n",
    "# df = generate_data()\n",
    "# df = df.withColumn(\"timestamp\", lit(datetime.datetime.now()))  # Add a timestamp column\n",
    "# df.write.format(\"delta\").mode(\"append\").save(\"/mnt/delta/raw_table\")\n",
    "# df.write.format(\"delta\").mode(\"append\").partitionBy(\"timestamp\").saveAsTable(\"tabular.dataexpert.jw_raw_example_table\")\n",
    "@dlt.view(\n",
    "    name=\"example_view\"\n",
    ")\n",
    "def create_example_view():\n",
    "    return generate_data()\n",
    "\n",
    "# # Define the Delta Live Table\n",
    "@dlt.table(\n",
    "    name=\"example_table\"\n",
    ")\n",
    "def create_example_table():\n",
    "    df = spark.read.table(\"example_view\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37af8d2c-a7ff-4cdd-9595-7176d2b142df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import dlt\n",
    "# from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# # Function to generate sample data\n",
    "# def generate_data():\n",
    "#     data = [\n",
    "#         (1, \"A\"),\n",
    "#         (2, \"B\"),\n",
    "#         (3, \"C\")\n",
    "#     ]\n",
    "#     df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "#     df = df.withColumn(\"timestamp\", current_timestamp())\n",
    "#     return df\n",
    "\n",
    "# # Define the Delta Live Table\n",
    "# @dlt.table(\n",
    "#     name=\"example_table\",\n",
    "#     comment=\"A simple example table\",\n",
    "#     table_properties={\"pipelines.appendOnly\": \"true\"}\n",
    "# )\n",
    "# def create_example_table():\n",
    "#     return generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b154556-8630-4283-9321-085dd613eed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.table(\"tabular.dataexpert.example_table\")\n",
    "# df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5efd0d6d-9eeb-439b-85de-8e3455ff380a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define the Delta Live Table for the master table\n",
    "# @dlt.table(\n",
    "#     name=\"example_table\",\n",
    "#     comment=\"Master table that appends new data\"\n",
    "# )\n",
    "# def update_master_table():\n",
    "#     # Read the existing master table\n",
    "#     existing_df = spark.read.table(\"tabular.dataexpert.example_table\")\n",
    "    \n",
    "#     # Read the new data from the temporary table\n",
    "#     new_df = spark.read.table(\"LIVE.example_table_temp\")\n",
    "    \n",
    "#     # Append new data to the existing data\n",
    "#     combined_df = existing_df.unionByName(new_df, allowMissingColumns=True)\n",
    "    \n",
    "#     return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b71d01f-8ec2-4053-b4d8-bf9c5f3b1f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # List all tables in the current catalog and schema\n",
    "# tables = spark.sql(\"SHOW TABLES IN tabular.dataexpert\")\n",
    "# display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004b0d6a-afce-41d3-a0d4-8c703c4607fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8268044402672728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dummy_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
