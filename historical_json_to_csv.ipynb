{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0e3604-4aa8-40b0-983b-952444739d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e147ddb7-42a8-41d0-aa3f-22cd7b85c4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the raw data from a .json file into a dataframe and set column names\n",
    "\n",
    "current_year = 2024\n",
    "\n",
    "file_name = f'/Volumes/tabular/dataexpert/jw_capstone/{current_year}_all_advgames.json'\n",
    "\n",
    "# Load the raw JSON data from the file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Note: Column names provided per Bart Torvik email communication (February 2025)\n",
    "column_names = [\n",
    "    \"numdate\", \"datetext\", \"opstyle\", \"quality\", \"win1\", \"opponent\", \"muid\", \"win2\",\n",
    "    \"Min_per\", \"ORtg\", \"Usage\", \"eFG\", \"TS_per\", \"ORB_per\", \"DRB_per\", \"AST_per\", \"TO_per\",\n",
    "    \"dunksmade\", \"dunksatt\", \"rimmade\", \"rimatt\", \"midmade\", \"midatt\", \"twoPM\", \"twoPA\", \n",
    "    \"TPM\", \"TPA\", \"FTM\", \"FTA\", \"bpm_rd\", \"Obpm\", \"Dbpm\", \"bpm_net\", \"pts\", \"ORB\", \"DRB\", \n",
    "    \"AST\", \"TOV\", \"STL\", \"BLK\", \"stl_per\", \"blk_per\", \"PF\", \"possessions\", \"bpm\", \"sbpm\", \n",
    "    \"loc\", \"tt\", \"pp\", \"inches\", \"cls\", \"pid\", \"year\"\n",
    "]\n",
    "\n",
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96fb85b-f324-4c72-bafc-a9e7866cad31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset exploration and primitive data quality checks\n",
    "\n",
    "# Check for columns with null values (NaN, None, etc.)\n",
    "columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "print(f\"columns with NaNs: {columns_with_nans}\")\n",
    "\n",
    "# Check which numeric columns have negative values\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "columns_with_negatives = numeric_columns[(df[numeric_columns] < 0).any()].tolist()\n",
    "\n",
    "# Define columns in which it is permissable to have negative values\n",
    "known_negatives = ['bpm_rd', 'Obpm', 'Dbpm', 'bpm_net', 'bpm', 'sbpm']\n",
    "columns_with_negatives = [col for col in columns_with_negatives if col not in known_negatives]\n",
    "\n",
    "# Highlight columns with \"surprise\" negative values\n",
    "print(f\"columns with surprise negative values: {columns_with_negatives}\")\n",
    "\n",
    "# Manually, yet systematically inspect each column's unique values\n",
    "offset = 1\n",
    "column_num = 20 - offset\n",
    "print(df.columns[column_num])\n",
    "print(str(len(df.iloc[:, column_num].unique())) + \" unique values (sorted):\")\n",
    "sorted(df.iloc[:, column_num].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df77c02-a4f8-4ee3-b7dd-ba899f9ae049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform basic data cleanup actions\n",
    "\n",
    "# Fill NaN and None values in known problematic columns\n",
    "df['cls'] = df['cls'].fillna('unknown')\n",
    "df['inches'] = df['inches'].fillna(0)\n",
    "\n",
    "# Fill negative values with 0 in known problematic columns\n",
    "columns_to_fill = ['Usage', 'AST_per', 'TO_per', 'twoPM', 'twoPA']\n",
    "for col in columns_to_fill:\n",
    "    df.loc[df[col] < 0, col] = 0\n",
    "\n",
    "# Drop unwanted columns\n",
    "columns_to_drop = ['opstyle', 'quality', 'loc', 'datetext', 'muid']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ce968-7d05-4347-a14d-39b35d88fdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality check: Deal with players that have multiple player IDs\n",
    "\n",
    "# Group by player name and count unique player IDs and teams\n",
    "grouped = df.groupby('pp').agg(\n",
    "    unique_player_ids=('pid', 'nunique'),\n",
    "    unique_teams=('tt', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Filter to show only player names with multiple unique player IDs\n",
    "players_with_multiple_ids = grouped[grouped['unique_player_ids'] > 1]\n",
    "\n",
    "# Confirm that the number of unique player IDs matches the number of unique teams\n",
    "result = players_with_multiple_ids[players_with_multiple_ids['unique_player_ids'] == players_with_multiple_ids['unique_teams']]\n",
    "\n",
    "# Calculate raww delta between number of players names and player ids in the overall dataset\n",
    "delta = len(df['pid'].unique()) - len(df['pp'].unique())\n",
    "\n",
    "# Count number of players with multiple pids\n",
    "multi_pid_players = len(result)\n",
    "\n",
    "# Count \"bonus\" pids (i.e. (total pids for a player) - 2) from players with more than 2 pids\n",
    "bonus_pids = ((result[result['unique_player_ids'] > 2]['unique_player_ids']) - 2).sum()\n",
    "\n",
    "# First check that players with multiple pids have the same number of unique player IDs as the number of unique teams\n",
    "check1 = (result['unique_player_ids'] != result['unique_teams']).sum() == 0\n",
    "\n",
    "# Then check that the overall delta is equal to the number of players with more than 2 pids + any bonus pids\n",
    "check2 = delta - len(result) - bonus_pids == 0\n",
    "\n",
    "print(f\"Number of unique player names: {len(df['pp'].unique())}\")\n",
    "print(f\"Number of unique player ids: {len(df['pid'].unique())}\")\n",
    "print(f\"Delta: {delta}\")\n",
    "print(f\"Number of players with multiple pids: {len(result)}\")\n",
    "(result['unique_player_ids'] != result['unique_teams']).sum()\n",
    "print(f\"Total 'bonus' pids from players with more than 2 pids: {bonus_pids}\")\n",
    "\n",
    "if check1 and check2:\n",
    "    print(\"DQC passed!  Delta b/t number of players names and player ids is acccounted for.\")\n",
    "else:\n",
    "    print(\"ERROR: Delta between delta and number of players with multiple pids is NOT acccounted for!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24939adc-4582-453f-8e6f-50bea6ee0126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot historgram of dataframe column\n",
    "\n",
    "col = 'Usage'\n",
    "df[col].hist(bins=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf1b56f-3273-49e4-929a-2aa09fe2d190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a .csv file for a given year\n",
    "\n",
    "year = current_year\n",
    "path = f\"/Volumes/tabular/dataexpert/jw_capstone/clean_csv/{year}_all_advgames_cleaned.csv\"\n",
    "df.to_csv(path)\n",
    "print(f'df for {year} saved to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e78ddae-6bfb-4852-823e-6eb82152ef65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect number of rows in each raw csv data file\n",
    "# TODO: Rewrite this using glob (or equivalent) so this doesn't depend on manual input\n",
    "\n",
    "rows = 0\n",
    "for i in range(0,11):\n",
    "    year = str(2014 + i)\n",
    "\n",
    "    file_path = f\"/Volumes/tabular/dataexpert/jw_capstone/clean_csv/{year}_all_advgames_cleaned.csv\"\n",
    "    print(file_path)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(len(df))\n",
    "    if df.shape[1] != 49:\n",
    "        print(df.shape[1])\n",
    "        print(\"ERROR: incorrect number of columns\")\n",
    "        break\n",
    "\n",
    "    rows = rows + len(df)\n",
    "\n",
    "print(f\"Total rows: {rows}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "historical_json_to_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
